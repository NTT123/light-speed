{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the process of converting text to speech, accomplished through the following two main steps:\n",
    "1. Predicting the phoneme durations using a Duration model.\n",
    "2. Generating the corresponding waveform using the VITS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # isort:skip\n",
    "from tfloader import load_tfdata\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "from models import SynthesizerTrn, DurationNet\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global config\n",
    "config_file = \"config.json\"\n",
    "duration_model_path = \"ckpts/duration_model.pth\"\n",
    "vits_model_path = \"ckpts/generator.pth\"\n",
    "output_file = \"clip.wav\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "with open(config_file, \"rb\") as f:\n",
    "    hps = json.load(f, object_hook=lambda x: SimpleNamespace(**x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an example test data\n",
    "ds = load_tfdata(\"data/tfdata\", \"test\", 1, 0, 0, 1).as_numpy_iterator()\n",
    "batch = next(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict phoneme duration\n",
    "duration_net = DurationNet(hps.data.vocab_size, 64, 4).to(device)\n",
    "duration_net.load_state_dict(torch.load(duration_model_path, map_location=device))\n",
    "duration_net = duration_net.eval()\n",
    "phone_length = torch.from_numpy(batch[\"phone_length\"].copy()).long().to(device)\n",
    "phone_idx = torch.from_numpy(batch[\"phone_idx\"].copy()).long().to(device)\n",
    "with torch.inference_mode():\n",
    "    phone_duration = duration_net(phone_idx, phone_length)[:, :,  0] * 1000\n",
    "\n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(phone_duration[0])\n",
    "plt.ylabel(\"duration (ms)\")\n",
    "plt.xlabel(\"phoneme index\")\n",
    "plt.title(\"predicted phoneme duration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = SynthesizerTrn(\n",
    "    hps.data.vocab_size, \n",
    "    hps.data.filter_length // 2 + 1, \n",
    "    hps.train.segment_size // hps.data.hop_length, \n",
    "    **vars(hps.model)\n",
    ").to(device)\n",
    "ckpt = torch.load(vits_model_path, map_location=device)\n",
    "params = {}\n",
    "for k, v in ckpt[\"net_g\"].items():\n",
    "    k = k[7:] if k.startswith(\"module.\") else k\n",
    "    params[k] = v\n",
    "generator.load_state_dict(params)\n",
    "del ckpt, params\n",
    "generator = generator.eval()\n",
    "end_time = torch.cumsum(phone_duration, dim=-1)\n",
    "start_time = end_time - phone_duration\n",
    "start_frame = (start_time * hps.data.sampling_rate / hps.data.hop_length / 1000).int()\n",
    "end_frame = (end_time * hps.data.sampling_rate / hps.data.hop_length / 1000).int()\n",
    "spec_length = end_frame.max(dim=-1).values\n",
    "pos = torch.arange(0, spec_length.item(), device=device)\n",
    "attn = torch.logical_and(\n",
    "    pos[None, :, None] >= start_frame[:, None, :], \n",
    "    pos[None, :, None] < end_frame[:, None, :]\n",
    ").float()\n",
    "with torch.inference_mode():\n",
    "    y_hat = generator.infer(phone_idx, phone_length, spec_length, attn, max_len=1000, noise_scale=0.667)[0]\n",
    "y_hat = y_hat[0, 0].data.cpu().numpy()\n",
    "wavfile.write(output_file, hps.data.sampling_rate, y_hat)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(y_hat)\n",
    "plt.xlim(0, len(y_hat))\n",
    "plt.title(\"generated waveform\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(attn[0].T, interpolation=\"nearest\")\n",
    "plt.title(\"attention map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
